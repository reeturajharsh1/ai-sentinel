{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b538d3a",
   "metadata": {},
   "source": [
    "# `OpenAI` client\n",
    "\n",
    "This notebook demonstrates how to instantiate the OpenAI object. OpenAI is a subclass of the BaseLLMClient Abstract class.\n",
    "\n",
    "The OpenAI Client works using an OpenAI compatible server. An OpenAI-compatible server refers to an inference server designed to host and serve large language models (LLMs) while exposing an API endpoint that mimics the structure and functionality of OpenAI's official API. \n",
    "\n",
    "An example of such servers are vLLM or Ollama.\n",
    "\n",
    "For more thorough look at OpenAIClient's structure, look to the [OpenAIClient](../../../docs/build/html/_autosummary/ai_sentinel.llm.OpenAIClient.html) or [BaseLLMClient](../../../docs/build/html/_autosummary/ai_sentinel.llm.BaseLLMClient.html) API reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3c319",
   "metadata": {},
   "source": [
    "### Setting up the Server\n",
    "Before the Client can be instantiated the server the open source LLM will be hosted on must be setup for the Client to function and be able to communicate with the desired LLM.\n",
    "\n",
    "There are various ways to instantiate the server depending on which application is being used to create it. The recommended method is using [vLLM](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html), though for smaller use cases [Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) can be used.\n",
    "\n",
    "For a more thorough explanation of server usage look to their respective documentation, linked above, for basic usage however an example will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118aff3",
   "metadata": {},
   "source": [
    "#### Run these command in a Linux terminal to set up vLLM server for OpenAIClient\n",
    "\n",
    "Installation:\n",
    "\n",
    "`uv pip install vllm`\n",
    "    \n",
    "vLLM OpenAI-compatible server setup:\n",
    "\n",
    "`vllm serve \"Qwen/Qwen3-8B\"`\n",
    "\n",
    "Once the server is set up, the server by default starts at http://localhost:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68626777",
   "metadata": {},
   "source": [
    "### Setting up the Client\n",
    "To set up the OpenAI Client there are 2 required parameters, the server Base URL, which is set up in the server setup. As well as the desired open source model (this should be the same model as given in the server setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_sentinel import OpenAIClient\n",
    "\n",
    "# Initialize an Open Source LLM Client\n",
    "client = OpenAIClient( \n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=\"Qwen/Qwen3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8d902",
   "metadata": {},
   "source": [
    "Once the Client is set up, all further usage in the code will be with the [ToxicityGuard](example_notebooks/api_examples/guards/basic_usage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918e04e",
   "metadata": {},
   "source": [
    "## Environment variable names\n",
    "The following is the recommended format to follow for key naming convention when working with Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY='YOUR_API_KEY'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
